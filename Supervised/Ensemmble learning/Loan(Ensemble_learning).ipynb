{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d61a547a-2a85-4ff1-a216-c4600de16890",
   "metadata": {},
   "source": [
    "## Ensemble learing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede4cf96-0b42-417f-9e2f-38e37268f390",
   "metadata": {},
   "source": [
    "### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37f3397c-b717-4d18-987f-1e9ada9dc2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression Cross-validated accuracy: 0.84\n",
      "DecisionTreeClassifier Cross-validated accuracy: 0.74\n",
      "ExtraTreesClassifier Cross-validated accuracy: 0.84\n",
      "SVC Cross-validated accuracy: 0.84\n",
      "Hard Voting Classifier Cross-validated accuracy: 0.84\n",
      "Soft Voting Classifier Cross-validated accuracy: 0.84\n",
      "Weighted Voting (weights=1,1,1) accuracy: 0.82\n",
      "Weighted Voting (weights=1,1,2) accuracy: 0.83\n",
      "Weighted Voting (weights=1,1,3) accuracy: 0.84\n",
      "Weighted Voting (weights=1,2,1) accuracy: 0.73\n",
      "Weighted Voting (weights=1,2,2) accuracy: 0.78\n",
      "Weighted Voting (weights=1,2,3) accuracy: 0.82\n",
      "Weighted Voting (weights=1,3,1) accuracy: 0.73\n",
      "Weighted Voting (weights=1,3,2) accuracy: 0.73\n",
      "Weighted Voting (weights=1,3,3) accuracy: 0.76\n",
      "Weighted Voting (weights=2,1,1) accuracy: 0.84\n",
      "Weighted Voting (weights=2,1,2) accuracy: 0.84\n",
      "Weighted Voting (weights=2,1,3) accuracy: 0.84\n",
      "Weighted Voting (weights=2,2,1) accuracy: 0.79\n",
      "Weighted Voting (weights=2,2,2) accuracy: 0.82\n",
      "Weighted Voting (weights=2,2,3) accuracy: 0.83\n",
      "Weighted Voting (weights=2,3,1) accuracy: 0.74\n",
      "Weighted Voting (weights=2,3,2) accuracy: 0.76\n",
      "Weighted Voting (weights=2,3,3) accuracy: 0.8\n",
      "Weighted Voting (weights=3,1,1) accuracy: 0.84\n",
      "Weighted Voting (weights=3,1,2) accuracy: 0.84\n",
      "Weighted Voting (weights=3,1,3) accuracy: 0.84\n",
      "Weighted Voting (weights=3,2,1) accuracy: 0.82\n",
      "Weighted Voting (weights=3,2,2) accuracy: 0.83\n",
      "Weighted Voting (weights=3,2,3) accuracy: 0.83\n",
      "Weighted Voting (weights=3,3,1) accuracy: 0.76\n",
      "Weighted Voting (weights=3,3,2) accuracy: 0.8\n",
      "Weighted Voting (weights=3,3,3) accuracy: 0.82\n",
      "SVC Cross-validated accuracy: 0.84\n",
      "SVC Cross-validated accuracy: 0.84\n",
      "SVC Cross-validated accuracy: 0.84\n",
      "SVM Soft Voting Classifier Cross-validated accuracy: 0.84\n",
      "Test set accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('loan_data.csv')  # Update with your actual path\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Create a LabelEncoder object\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode categorical features\n",
    "for col in categorical_cols:\n",
    "    df[col] = label_encoder.fit_transform(df[col])\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop('not.fully.paid', axis=1)  # Features\n",
    "y = df['not.fully.paid']  # Target variable\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define classifiers with increased max_iter for Logistic Regression\n",
    "clf1 = LogisticRegression(max_iter=200)  # Increase max_iter\n",
    "clf2 = DecisionTreeClassifier()\n",
    "clf3 = ExtraTreesClassifier()\n",
    "clf4 = SVC(probability=True)\n",
    "\n",
    "# Evaluating each classifier individually\n",
    "classifiers = [clf1, clf2, clf3, clf4]\n",
    "for clf in classifiers:\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    print(f\"{clf.__class__.__name__} Cross-validated accuracy: {np.round(np.mean(scores), 2)}\")\n",
    "\n",
    "# Hard Voting\n",
    "hard_voting_clf = VotingClassifier(estimators=[('lr', clf1), ('dt', clf2), ('et', clf3)], voting='hard')\n",
    "hard_voting_scores = cross_val_score(hard_voting_clf, X_train, y_train, cv=10, scoring='accuracy')\n",
    "print(\"Hard Voting Classifier Cross-validated accuracy:\", np.round(np.mean(hard_voting_scores), 2))\n",
    "\n",
    "# Soft Voting\n",
    "soft_voting_clf = VotingClassifier(estimators=[('lr', clf1), ('dt', clf2), ('et', clf3), ('svc', clf4)], voting='soft')\n",
    "soft_voting_scores = cross_val_score(soft_voting_clf, X_train, y_train, cv=10, scoring='accuracy')\n",
    "print(\"Soft Voting Classifier Cross-validated accuracy:\", np.round(np.mean(soft_voting_scores), 2))\n",
    "\n",
    "# Weighted Voting\n",
    "for i in range(1, 4):\n",
    "    for j in range(1, 4):\n",
    "        for k in range(1, 4):\n",
    "            weighted_voting_clf = VotingClassifier(estimators=[('lr', clf1), ('dt', clf2), ('et', clf3)], \n",
    "                                                    voting='soft', \n",
    "                                                    weights=[i, j, k])\n",
    "            weighted_scores = cross_val_score(weighted_voting_clf, X_train, y_train, cv=10, scoring='accuracy')\n",
    "            print(f\"Weighted Voting (weights={i},{j},{k}) accuracy:\", np.round(np.mean(weighted_scores), 2))\n",
    "\n",
    "# Classifiers of the same algorithm\n",
    "# Using SVC with different kernels\n",
    "svm1 = SVC(probability=True, kernel='linear')\n",
    "svm2 = SVC(probability=True, kernel='poly', degree=2)\n",
    "svm3 = SVC(probability=True, kernel='rbf')\n",
    "\n",
    "svm_classifiers = [svm1, svm2, svm3]\n",
    "for svm in svm_classifiers:\n",
    "    svm_scores = cross_val_score(svm, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    print(f\"{svm.__class__.__name__} Cross-validated accuracy: {np.round(np.mean(svm_scores), 2)}\")\n",
    "\n",
    "# Soft Voting with SVM classifiers\n",
    "svm_voting_clf = VotingClassifier(estimators=[('svm1', svm1), ('svm2', svm2), ('svm3', svm3)], voting='soft')\n",
    "svm_voting_scores = cross_val_score(svm_voting_clf, X_train, y_train, cv=10, scoring='accuracy')\n",
    "print(\"SVM Soft Voting Classifier Cross-validated accuracy:\", np.round(np.mean(svm_voting_scores), 2))\n",
    "\n",
    "# Fit the best model and predict\n",
    "best_model = soft_voting_clf  # Choose based on your evaluation\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy on test set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test set accuracy:\", np.round(accuracy, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425e886c-a920-49be-bd2e-7f394984d1eb",
   "metadata": {},
   "source": [
    "### Voting (regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ccec1ee-7872-4a80-99f2-1346e821b604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_regression - Cross-validated R²: 0.65\n",
      "decision_tree - Cross-validated R²: 0.5\n",
      "extra_trees - Cross-validated R²: 0.74\n",
      "support_vector_regression - Cross-validated R²: -0.34\n",
      "Voting Regressor - Cross-validated R²: 0.65\n",
      "linear_regression - Train Set MSE: 0.0, R²: 0.65\n",
      "linear_regression - Test Set MSE: 0.0, R²: 0.64\n",
      "\n",
      "decision_tree - Train Set MSE: 0.0, R²: 1.0\n",
      "decision_tree - Test Set MSE: 0.0, R²: 0.52\n",
      "\n",
      "extra_trees - Train Set MSE: 0.0, R²: 1.0\n",
      "extra_trees - Test Set MSE: 0.0, R²: 0.74\n",
      "\n",
      "support_vector_regression - Train Set MSE: 0.0, R²: -0.34\n",
      "support_vector_regression - Test Set MSE: 0.0, R²: -0.32\n",
      "\n",
      "Average Predictions Train Set MSE: 0.0\n",
      "Average Predictions Train Set R²: 0.85\n",
      "Average Predictions Test Set MSE: 0.0\n",
      "Average Predictions Test Set R²: 0.66\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor, VotingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Load the dataset\n",
    "dataframe = pd.read_csv('loan_data.csv')  # Update with your actual path\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_columns = dataframe.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Create a LabelEncoder object\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode categorical features\n",
    "for column in categorical_columns:\n",
    "    dataframe[column] = label_encoder.fit_transform(dataframe[column])\n",
    "\n",
    "# Separate features and target variable\n",
    "features = dataframe.drop('int.rate', axis=1)  # Features\n",
    "target = dataframe['int.rate']  # Target variable\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "features_train, features_test, target_train, target_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define regressors for voting\n",
    "estimators = [\n",
    "    ('linear_regression', LinearRegression()),\n",
    "    ('decision_tree', DecisionTreeRegressor()),\n",
    "    ('extra_trees', ExtraTreesRegressor()),\n",
    "    ('support_vector_regression', SVR())\n",
    "]\n",
    "\n",
    "# Evaluate individual regressors\n",
    "for name, regressor in estimators:\n",
    "    scores = cross_val_score(regressor, features_train, target_train, scoring='r2', cv=10)\n",
    "    print(f\"{name} - Cross-validated R²: {np.round(np.mean(scores), 2)}\")\n",
    "\n",
    "# Voting Regressor\n",
    "voting_regressor = VotingRegressor(estimators)\n",
    "voting_regressor.fit(features_train, target_train)\n",
    "voting_scores = cross_val_score(voting_regressor, features_train, target_train, scoring='r2', cv=10)\n",
    "print(\"Voting Regressor - Cross-validated R²:\", np.round(np.mean(voting_scores), 2))\n",
    "\n",
    "# Fit each regressor and get predictions\n",
    "predictions_train = []\n",
    "predictions_test = []\n",
    "\n",
    "for name, regressor in estimators:\n",
    "    regressor.fit(features_train, target_train)\n",
    "    target_train_predicted = regressor.predict(features_train)\n",
    "    target_test_predicted = regressor.predict(features_test)\n",
    "\n",
    "    # Store predictions for averaging\n",
    "    predictions_train.append(target_train_predicted)\n",
    "    predictions_test.append(target_test_predicted)\n",
    "\n",
    "    # Calculate and display metrics\n",
    "    mse_train = mean_squared_error(target_train, target_train_predicted)\n",
    "    r2_train = r2_score(target_train, target_train_predicted)\n",
    "    mse_test = mean_squared_error(target_test, target_test_predicted)\n",
    "    r2_test = r2_score(target_test, target_test_predicted)\n",
    "\n",
    "    print(f\"{name} - Train Set MSE: {np.round(mse_train, 2)}, R²: {np.round(r2_train, 2)}\")\n",
    "    print(f\"{name} - Test Set MSE: {np.round(mse_test, 2)}, R²: {np.round(r2_test, 2)}\\n\")\n",
    "\n",
    "# Average predictions for train and test set\n",
    "average_train_prediction = np.mean(predictions_train, axis=0)\n",
    "average_test_prediction = np.mean(predictions_test, axis=0)\n",
    "\n",
    "# Calculate MSE and R² for averaged predictions\n",
    "mse_average_train = mean_squared_error(target_train, average_train_prediction)\n",
    "r2_average_train = r2_score(target_train, average_train_prediction)\n",
    "\n",
    "mse_average_test = mean_squared_error(target_test, average_test_prediction)\n",
    "r2_average_test = r2_score(target_test, average_test_prediction)\n",
    "\n",
    "print(\"Average Predictions Train Set MSE:\", np.round(mse_average_train, 2))\n",
    "print(\"Average Predictions Train Set R²:\", np.round(r2_average_train, 2))\n",
    "print(\"Average Predictions Test Set MSE:\", np.round(mse_average_test, 2))\n",
    "print(\"Average Predictions Test Set R²:\", np.round(r2_average_test, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153d99e9-5e5a-41f1-82df-67c6fdc813d0",
   "metadata": {},
   "source": [
    "## Bagging (Regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519391f9-b3ce-4ebe-9f63-ce145408ad4d",
   "metadata": {},
   "source": [
    " ### Simple linear Regression (MSE check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fa67742-9e04-4905-8419-916f5cec590d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0\n",
      "R^2 Score: 0.64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "dataframe = pd.read_csv('loan_data.csv')  # Update with your actual path\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_columns = dataframe.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Create a LabelEncoder object\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode categorical features\n",
    "for column in categorical_columns:\n",
    "    dataframe[column] = label_encoder.fit_transform(dataframe[column])\n",
    "\n",
    "# Separate features and target variable\n",
    "features = dataframe.drop('int.rate', axis=1)  # Features\n",
    "target = dataframe['int.rate']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(features_train, target_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "target_pred = model.predict(features_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(target_test, target_pred)\n",
    "r2 = r2_score(target_test, target_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", np.round(mse, 2))\n",
    "print(\"R^2 Score:\", np.round(r2, 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37c1334e-a761-44a4-a448-0cc04e083157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression - Train Set MSE: 0.0, R²: 0.65\n",
      "Linear Regression - Test Set MSE: 0.0, R²: 0.64\n",
      "\n",
      "Decision Tree - Train Set MSE: 0.0, R²: 1.0\n",
      "Decision Tree - Test Set MSE: 0.0, R²: 0.48\n",
      "\n",
      "SVR - Train Set MSE: 0.0, R²: -0.34\n",
      "SVR - Test Set MSE: 0.0, R²: -0.32\n",
      "\n",
      "Bagging Regressor - Train Set MSE: 0.0\n",
      "Bagging Regressor - Train Set R²: 0.96\n",
      "Bagging Regressor - Test Set MSE: 0.0\n",
      "Bagging Regressor - Test Set R²: 0.74\n",
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
      "Best R² Score Through Grid Search: 0.741\n",
      "Best Parameters:  {'bootstrap': True, 'estimator': DecisionTreeRegressor(), 'max_features': 1.0, 'max_samples': 0.5, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Load the dataset\n",
    "dataframe = pd.read_csv('loan_data.csv')  # Update with your actual path\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_columns = dataframe.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Create a LabelEncoder object\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode categorical features\n",
    "for column in categorical_columns:\n",
    "    dataframe[column] = label_encoder.fit_transform(dataframe[column])\n",
    "\n",
    "# Separate features and target variable\n",
    "features = dataframe.drop('int.rate', axis=1)  # Features\n",
    "target = dataframe['int.rate']  # Target variable\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "features_train, features_test, target_train, target_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define base regressors\n",
    "regressors = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Decision Tree': DecisionTreeRegressor(),\n",
    "    'SVR': SVR()\n",
    "}\n",
    "\n",
    "# Fit each regressor and get predictions\n",
    "predictions_train = []\n",
    "predictions_test = []\n",
    "\n",
    "for name, reg in regressors.items():\n",
    "    reg.fit(features_train, target_train)\n",
    "    y_train_pred = reg.predict(features_train)\n",
    "    y_test_pred = reg.predict(features_test)\n",
    "    \n",
    "    # Store predictions for averaging\n",
    "    predictions_train.append(y_train_pred)\n",
    "    predictions_test.append(y_test_pred)\n",
    "\n",
    "    # Calculate and display metrics\n",
    "    mse_train = mean_squared_error(target_train, y_train_pred)\n",
    "    r2_train = r2_score(target_train, y_train_pred)\n",
    "    mse_test = mean_squared_error(target_test, y_test_pred)\n",
    "    r2_test = r2_score(target_test, y_test_pred)\n",
    "\n",
    "    print(f\"{name} - Train Set MSE: {np.round(mse_train, 2)}, R²: {np.round(r2_train, 2)}\")\n",
    "    print(f\"{name} - Test Set MSE: {np.round(mse_test, 2)}, R²: {np.round(r2_test, 2)}\\n\")\n",
    "\n",
    "# Bagging Regressor with Decision Tree\n",
    "bag_regressor = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=100, random_state=1)\n",
    "bag_regressor.fit(features_train, target_train)\n",
    "\n",
    "# Predictions with Bagging Regressor\n",
    "y_bag_pred_train = bag_regressor.predict(features_train)\n",
    "y_bag_pred_test = bag_regressor.predict(features_test)\n",
    "\n",
    "# Calculate metrics for Bagging Regressor\n",
    "mse_bag_train = mean_squared_error(target_train, y_bag_pred_train)\n",
    "r2_bag_train = r2_score(target_train, y_bag_pred_train)\n",
    "mse_bag_test = mean_squared_error(target_test, y_bag_pred_test)\n",
    "r2_bag_test = r2_score(target_test, y_bag_pred_test)\n",
    "\n",
    "print(\"Bagging Regressor - Train Set MSE:\", np.round(mse_bag_train, 2))\n",
    "print(\"Bagging Regressor - Train Set R²:\", np.round(r2_bag_train, 2))\n",
    "print(\"Bagging Regressor - Test Set MSE:\", np.round(mse_bag_test, 2))\n",
    "print(\"Bagging Regressor - Test Set R²:\", np.round(r2_bag_test, 2))\n",
    "\n",
    "# Grid Search for Best Bagging Regressor\n",
    "params = {\n",
    "    'estimator': [LinearRegression(), DecisionTreeRegressor(), SVR()],\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_samples': [0.5, 1.0],\n",
    "    'max_features': [0.5, 1.0],\n",
    "    'bootstrap': [True, False],\n",
    "}\n",
    "\n",
    "bagging_regressor_grid = GridSearchCV(BaggingRegressor(random_state=1), param_grid=params, cv=3, n_jobs=-1, verbose=1)\n",
    "bagging_regressor_grid.fit(features_train, target_train)\n",
    "\n",
    "print('Best R² Score Through Grid Search: %.3f' % bagging_regressor_grid.best_score_)\n",
    "print('Best Parameters: ', bagging_regressor_grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b28d9c5-9441-43c0-a850-7619318e587b",
   "metadata": {},
   "source": [
    "## Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fe46c94-8c7f-40c0-96b2-b4b4f9fb2125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Train Accuracy: 0.84, Test Accuracy: 0.84\n",
      "\n",
      "Decision Tree - Train Accuracy: 1.0, Test Accuracy: 0.76\n",
      "\n",
      "K-Nearest Neighbors - Train Accuracy: 0.85, Test Accuracy: 0.83\n",
      "\n",
      "Naive Bayes - Train Accuracy: 0.79, Test Accuracy: 0.79\n",
      "\n",
      "Random Forest - Train Accuracy: 1.0, Test Accuracy: 0.84\n",
      "\n",
      "Bagging Classifier - Train Accuracy: 1.0\n",
      "Bagging Classifier - Test Accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Load the dataset\n",
    "dataframe = pd.read_csv('loan_data.csv')  # Update with your actual path\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_columns = dataframe.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Create a LabelEncoder object\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode categorical features\n",
    "for column in categorical_columns:\n",
    "    dataframe[column] = label_encoder.fit_transform(dataframe[column])\n",
    "\n",
    "# Separate features and target variable\n",
    "features = dataframe.drop('not.fully.paid', axis=1)  # Features\n",
    "target = dataframe['not.fully.paid']  # Target variable\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "features_train, features_test, target_train, target_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define base classifiers\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Random Forest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "# Fit each classifier and get predictions\n",
    "predictions_train = []\n",
    "predictions_test = []\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(features_train, target_train)\n",
    "    y_train_pred = clf.predict(features_train)\n",
    "    y_test_pred = clf.predict(features_test)\n",
    "    \n",
    "    # Store predictions for potential averaging\n",
    "    predictions_train.append(y_train_pred)\n",
    "    predictions_test.append(y_test_pred)\n",
    "\n",
    "    # Calculate and display metrics\n",
    "    train_accuracy = accuracy_score(target_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(target_test, y_test_pred)\n",
    "\n",
    "    print(f\"{name} - Train Accuracy: {np.round(train_accuracy, 2)}, Test Accuracy: {np.round(test_accuracy, 2)}\\n\")\n",
    "\n",
    "# Bagging Classifier with Decision Tree\n",
    "bag_classifier = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, random_state=1)\n",
    "bag_classifier.fit(features_train, target_train)\n",
    "\n",
    "# Predictions with Bagging Classifier\n",
    "y_bag_pred_train = bag_classifier.predict(features_train)\n",
    "y_bag_pred_test = bag_classifier.predict(features_test)\n",
    "\n",
    "# Calculate accuracy for Bagging Classifier\n",
    "train_accuracy_bag = accuracy_score(target_train, y_bag_pred_train)\n",
    "test_accuracy_bag = accuracy_score(target_test, y_bag_pred_test)\n",
    "\n",
    "print(\"Bagging Classifier - Train Accuracy:\", np.round(train_accuracy_bag, 2))\n",
    "print(\"Bagging Classifier - Test Accuracy:\", np.round(test_accuracy_bag, 2))\n",
    "\n",
    "# # Grid Search for Best Bagging Classifier\n",
    "# params = {\n",
    "#     'estimator': [LogisticRegression(), DecisionTreeClassifier(), KNeighborsClassifier(), GaussianNB(), RandomForestClassifier()],\n",
    "#     'n_estimators': [50, 100],\n",
    "#     'max_samples': [0.5, 1.0],\n",
    "#     'max_features': [0.5, 1.0],\n",
    "#     'bootstrap': [True, False],\n",
    "# }\n",
    "\n",
    "# bagging_classifier_grid = GridSearchCV(BaggingClassifier(random_state=1), param_grid=params, cv=3, n_jobs=-1, verbose=1)\n",
    "# bagging_classifier_grid.fit(features_train, target_train)\n",
    "\n",
    "# print('Best Accuracy Score Through Grid Search: %.3f' % bagging_classifier_grid.best_score_)\n",
    "# print('Best Parameters: ', bagging_classifier_grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecc26b6-36df-4f52-976b-0d971da65c2d",
   "metadata": {},
   "source": [
    "## Comparison of Bagging Classifier and Random Forest Classifier\n",
    "\n",
    "### Bagging Classifier\n",
    "- **Definition**: Stands for \"Bootstrap Aggregating.\" Creates multiple copies of training data by sampling with replacement.\n",
    "- **Purpose**: Reduces variance and improves accuracy by combining predictions from multiple models.\n",
    "- **Model Type**: Can use any kind of model (e.g., decision trees, linear models).\n",
    "- **Decision Making**: Final prediction is made by averaging (for regression) or voting (for classification) from all models.\n",
    "\n",
    "### Random Forest Classifier\n",
    "- **Definition**: A specific type of Bagging that uses decision trees as base models and adds randomness in feature selection.\n",
    "- **Purpose**: Makes trees less correlated, improving overall performance.\n",
    "- **Model Type**: Primarily based on decision trees.\n",
    "- **Decision Making**: Combines predictions from multiple trees through voting or averaging.\n",
    "\n",
    "### Key Differences\n",
    "- **Model Variability**: Bagging can use various models, while Random Forest uses only decision trees.\n",
    "- **Feature Selection**: Random Forest selects a random subset of features for each tree, adding more randomness.\n",
    "- **Complexity**: Random Forest generally performs better than basic Bagging, especially with complex datasets.\n",
    "\n",
    "**Summary**: Bagging improves model performance by averaging predictions from multiple copies, while Random Forest is a specialized version that focuses on decision trees with added randomness for better results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12e11d17-e80d-4d3b-acd3-47e7006c972c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier - Train Accuracy: 1.0\n",
      "Bagging Classifier - Test Accuracy: 0.84\n",
      "Random Forest Classifier - Train Accuracy: 1.0\n",
      "Random Forest Classifier - Test Accuracy: 0.84\n",
      "\n",
      "Comparison of Classifiers:\n",
      "Bagging Classifier Test Accuracy: 0.84\n",
      "Random Forest Classifier Test Accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load the dataset\n",
    "dataframe = pd.read_csv('loan_data.csv')  # Update with your actual path\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_columns = dataframe.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Create a LabelEncoder object\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode categorical features\n",
    "for column in categorical_columns:\n",
    "    dataframe[column] = label_encoder.fit_transform(dataframe[column])\n",
    "\n",
    "# Separate features and target variable\n",
    "features = dataframe.drop('not.fully.paid', axis=1)  # Features\n",
    "target = dataframe['not.fully.paid']  # Target variable\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "features_train, features_test, target_train, target_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Bagging Classifier with Decision Tree\n",
    "bag_classifier = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, random_state=1)\n",
    "bag_classifier.fit(features_train, target_train)\n",
    "\n",
    "# Predictions with Bagging Classifier\n",
    "y_bag_pred_train = bag_classifier.predict(features_train)\n",
    "y_bag_pred_test = bag_classifier.predict(features_test)\n",
    "\n",
    "# Calculate accuracy for Bagging Classifier\n",
    "train_accuracy_bag = accuracy_score(target_train, y_bag_pred_train)\n",
    "test_accuracy_bag = accuracy_score(target_test, y_bag_pred_test)\n",
    "\n",
    "print(\"Bagging Classifier - Train Accuracy:\", np.round(train_accuracy_bag, 2))\n",
    "print(\"Bagging Classifier - Test Accuracy:\", np.round(test_accuracy_bag, 2))\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "rf_classifier.fit(features_train, target_train)\n",
    "\n",
    "# Predictions with Random Forest Classifier\n",
    "y_rf_pred_train = rf_classifier.predict(features_train)\n",
    "y_rf_pred_test = rf_classifier.predict(features_test)\n",
    "\n",
    "# Calculate accuracy for Random Forest Classifier\n",
    "train_accuracy_rf = accuracy_score(target_train, y_rf_pred_train)\n",
    "test_accuracy_rf = accuracy_score(target_test, y_rf_pred_test)\n",
    "\n",
    "print(\"Random Forest Classifier - Train Accuracy:\", np.round(train_accuracy_rf, 2))\n",
    "print(\"Random Forest Classifier - Test Accuracy:\", np.round(test_accuracy_rf, 2))\n",
    "\n",
    "# Comparison of accuracy\n",
    "print(f\"\\nComparison of Classifiers:\")\n",
    "print(f\"Bagging Classifier Test Accuracy: {np.round(test_accuracy_bag, 2)}\")\n",
    "print(f\"Random Forest Classifier Test Accuracy: {np.round(test_accuracy_rf, 2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df585e1-d130-4ea8-bed1-c6c68d3bd3a7",
   "metadata": {},
   "source": [
    "## heart data set randomforest vs bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8612e578-1e9f-44c0-ac87-014b73d25726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303 entries, 0 to 302\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       303 non-null    int64  \n",
      " 1   sex       303 non-null    int64  \n",
      " 2   cp        303 non-null    int64  \n",
      " 3   trestbps  303 non-null    int64  \n",
      " 4   chol      303 non-null    int64  \n",
      " 5   fbs       303 non-null    int64  \n",
      " 6   restecg   303 non-null    int64  \n",
      " 7   thalach   303 non-null    int64  \n",
      " 8   exang     303 non-null    int64  \n",
      " 9   oldpeak   303 non-null    float64\n",
      " 10  slope     303 non-null    int64  \n",
      " 11  ca        303 non-null    int64  \n",
      " 12  thal      303 non-null    int64  \n",
      " 13  target    303 non-null    int64  \n",
      "dtypes: float64(1), int64(13)\n",
      "memory usage: 33.3 KB\n",
      "None\n",
      "Bagging Classifier - Train Accuracy: 1.0\n",
      "Bagging Classifier - Test Accuracy: 0.84\n",
      "Random Forest Classifier - Train Accuracy: 1.0\n",
      "Random Forest Classifier - Test Accuracy: 0.84\n",
      "\n",
      "Comparison of Classifiers:\n",
      "Bagging Classifier Test Accuracy: 0.84\n",
      "Random Forest Classifier Test Accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load the dataset\n",
    "dataframe = pd.read_csv('heart.csv')  # Update with your actual path\n",
    "\n",
    "# Summary of the dataset\n",
    "print(dataframe.info())\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_columns = dataframe.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Create a LabelEncoder object\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode categorical features\n",
    "for column in categorical_columns:\n",
    "    dataframe[column] = label_encoder.fit_transform(dataframe[column])\n",
    "\n",
    "# Separate features and target variable\n",
    "features = dataframe.drop('target', axis=1)  # Features\n",
    "target = dataframe['target']  # Target variable\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "features_train, features_test, target_train, target_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Bagging Classifier with Decision Tree\n",
    "bag_classifier = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, random_state=1)\n",
    "bag_classifier.fit(features_train, target_train)\n",
    "\n",
    "# Predictions with Bagging Classifier\n",
    "y_bag_pred_train = bag_classifier.predict(features_train)\n",
    "y_bag_pred_test = bag_classifier.predict(features_test)\n",
    "\n",
    "# Calculate accuracy for Bagging Classifier\n",
    "train_accuracy_bag = accuracy_score(target_train, y_bag_pred_train)\n",
    "test_accuracy_bag = accuracy_score(target_test, y_bag_pred_test)\n",
    "\n",
    "print(\"Bagging Classifier - Train Accuracy:\", np.round(train_accuracy_bag, 2))\n",
    "print(\"Bagging Classifier - Test Accuracy:\", np.round(test_accuracy_bag, 2))\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "rf_classifier.fit(features_train, target_train)\n",
    "\n",
    "# Predictions with Random Forest Classifier\n",
    "y_rf_pred_train = rf_classifier.predict(features_train)\n",
    "y_rf_pred_test = rf_classifier.predict(features_test)\n",
    "\n",
    "# Calculate accuracy for Random Forest Classifier\n",
    "train_accuracy_rf = accuracy_score(target_train, y_rf_pred_train)\n",
    "test_accuracy_rf = accuracy_score(target_test, y_rf_pred_test)\n",
    "\n",
    "print(\"Random Forest Classifier - Train Accuracy:\", np.round(train_accuracy_rf, 2))\n",
    "print(\"Random Forest Classifier - Test Accuracy:\", np.round(test_accuracy_rf, 2))\n",
    "\n",
    "# Comparison of accuracy\n",
    "print(f\"\\nComparison of Classifiers:\")\n",
    "print(f\"Bagging Classifier Test Accuracy: {np.round(test_accuracy_bag, 2)}\")\n",
    "print(f\"Random Forest Classifier Test Accuracy: {np.round(test_accuracy_rf, 2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aa29ae-d054-4946-b6b2-0a2508991767",
   "metadata": {},
   "source": [
    "## Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7da6b11f-3265-46f2-841d-313ee88678fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\miniconda3\\envs\\machine_learning\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Classifier - Train Accuracy: 1.0\n",
      "AdaBoost Classifier - Test Accuracy: 0.75\n",
      "Gradient Boosting Classifier - Train Accuracy: 0.85\n",
      "Gradient Boosting Classifier - Test Accuracy: 0.84\n",
      "Histogram-based Gradient Boosting Classifier - Train Accuracy: 0.9\n",
      "Histogram-based Gradient Boosting Classifier - Test Accuracy: 0.84\n",
      "XGBoost Classifier - Train Accuracy: 0.96\n",
      "XGBoost Classifier - Test Accuracy: 0.83\n",
      "\n",
      "Comparison of Classifiers:\n",
      "AdaBoost Classifier Test Accuracy: 0.75\n",
      "Gradient Boosting Classifier Test Accuracy: 0.84\n",
      "Histogram-based Gradient Boosting Classifier Test Accuracy: 0.84\n",
      "XGBoost Classifier Test Accuracy: 0.83\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier  # Correct import for XGBoost\n",
    "\n",
    "# Load the dataset\n",
    "dataframe = pd.read_csv('loan_data.csv')  # Update with your actual path\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_columns = dataframe.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Create a LabelEncoder object\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode categorical features\n",
    "for column in categorical_columns:\n",
    "   dataframe[column] = label_encoder.fit_transform(dataframe[column])\n",
    "\n",
    "# Separate features and target variable\n",
    "features = dataframe.drop('not.fully.paid', axis=1)  # Features\n",
    "target = dataframe['not.fully.paid']  # Target variable\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "5\n",
    "# Split the data into training and testing sets\n",
    "features_train, features_test, target_train, target_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# AdaBoost Classifier\n",
    "ada_classifier = AdaBoostClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, random_state=1)\n",
    "ada_classifier.fit(features_train, target_train)\n",
    "\n",
    "# Predictions with AdaBoost Classifier\n",
    "y_ada_pred_train = ada_classifier.predict(features_train)\n",
    "y_ada_pred_test = ada_classifier.predict(features_test)\n",
    "\n",
    "# Calculate accuracy for AdaBoost Classifier\n",
    "train_accuracy_ada = accuracy_score(target_train, y_ada_pred_train)\n",
    "test_accuracy_ada = accuracy_score(target_test, y_ada_pred_test)\n",
    "\n",
    "print(\"AdaBoost Classifier - Train Accuracy:\", np.round(train_accuracy_ada, 2))\n",
    "print(\"AdaBoost Classifier - Test Accuracy:\", np.round(test_accuracy_ada, 2))\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "gb_classifier = GradientBoostingClassifier(n_estimators=100, random_state=1)\n",
    "gb_classifier.fit(features_train, target_train)\n",
    "\n",
    "# Predictions with Gradient Boosting Classifier\n",
    "y_gb_pred_train = gb_classifier.predict(features_train)\n",
    "y_gb_pred_test = gb_classifier.predict(features_test)\n",
    "\n",
    "# Calculate accuracy for Gradient Boosting Classifier\n",
    "train_accuracy_gb = accuracy_score(target_train, y_gb_pred_train)\n",
    "test_accuracy_gb = accuracy_score(target_test, y_gb_pred_test)\n",
    "\n",
    "print(\"Gradient Boosting Classifier - Train Accuracy:\", np.round(train_accuracy_gb, 2))\n",
    "print(\"Gradient Boosting Classifier - Test Accuracy:\", np.round(test_accuracy_gb, 2))\n",
    "\n",
    "# Histogram-based Gradient Boosting Classifier\n",
    "hist_gb_classifier = HistGradientBoostingClassifier(max_iter=100, random_state=1)\n",
    "hist_gb_classifier.fit(features_train, target_train)\n",
    "\n",
    "# Predictions with Histogram-based Gradient Boosting Classifier\n",
    "y_hist_gb_pred_train = hist_gb_classifier.predict(features_train)\n",
    "y_hist_gb_pred_test = hist_gb_classifier.predict(features_test)\n",
    "\n",
    "# Calculate accuracy for Histogram-based Gradient Boosting Classifier\n",
    "train_accuracy_hist_gb = accuracy_score(target_train, y_hist_gb_pred_train)\n",
    "test_accuracy_hist_gb = accuracy_score(target_test, y_hist_gb_pred_test)\n",
    "\n",
    "print(\"Histogram-based Gradient Boosting Classifier - Train Accuracy:\", np.round(train_accuracy_hist_gb, 2))\n",
    "print(\"Histogram-based Gradient Boosting Classifier - Test Accuracy:\", np.round(test_accuracy_hist_gb, 2))\n",
    "\n",
    "# XGBoost Classifier\n",
    "xgb_classifier = XGBClassifier(n_estimators=100, random_state=1)\n",
    "xgb_classifier.fit(features_train, target_train)\n",
    "\n",
    "# Predictions with XGBoost Classifier\n",
    "y_xgb_pred_train = xgb_classifier.predict(features_train)\n",
    "y_xgb_pred_test = xgb_classifier.predict(features_test)\n",
    "\n",
    "# Calculate accuracy for XGBoost Classifier\n",
    "train_accuracy_xgb = accuracy_score(target_train, y_xgb_pred_train)\n",
    "test_accuracy_xgb = accuracy_score(target_test, y_xgb_pred_test)\n",
    "\n",
    "print(\"XGBoost Classifier - Train Accuracy:\", np.round(train_accuracy_xgb, 2))\n",
    "print(\"XGBoost Classifier - Test Accuracy:\", np.round(test_accuracy_xgb, 2))\n",
    "\n",
    "# Comparison of accuracy\n",
    "print(f\"\\nComparison of Classifiers:\")\n",
    "print(f\"AdaBoost Classifier Test Accuracy: {np.round(test_accuracy_ada, 2)}\")\n",
    "print(f\"Gradient Boosting Classifier Test Accuracy: {np.round(test_accuracy_gb, 2)}\")\n",
    "print(f\"Histogram-based Gradient Boosting Classifier Test Accuracy: {np.round(test_accuracy_hist_gb, 2)}\")\n",
    "print(f\"XGBoost Classifier Test Accuracy: {np.round(test_accuracy_xgb, 2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47ca6f52-8d5e-427f-a001-336d2c5b7826",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>credit.policy</th>\n",
       "      <th>purpose</th>\n",
       "      <th>int.rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>log.annual.inc</th>\n",
       "      <th>dti</th>\n",
       "      <th>fico</th>\n",
       "      <th>days.with.cr.line</th>\n",
       "      <th>revol.bal</th>\n",
       "      <th>revol.util</th>\n",
       "      <th>inq.last.6mths</th>\n",
       "      <th>delinq.2yrs</th>\n",
       "      <th>pub.rec</th>\n",
       "      <th>not.fully.paid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>0.1189</td>\n",
       "      <td>829.10</td>\n",
       "      <td>11.350407</td>\n",
       "      <td>19.48</td>\n",
       "      <td>737</td>\n",
       "      <td>5639.958333</td>\n",
       "      <td>28854</td>\n",
       "      <td>52.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>0.1071</td>\n",
       "      <td>228.22</td>\n",
       "      <td>11.082143</td>\n",
       "      <td>14.29</td>\n",
       "      <td>707</td>\n",
       "      <td>2760.000000</td>\n",
       "      <td>33623</td>\n",
       "      <td>76.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>0.1357</td>\n",
       "      <td>366.86</td>\n",
       "      <td>10.373491</td>\n",
       "      <td>11.63</td>\n",
       "      <td>682</td>\n",
       "      <td>4710.000000</td>\n",
       "      <td>3511</td>\n",
       "      <td>25.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>0.1008</td>\n",
       "      <td>162.34</td>\n",
       "      <td>11.350407</td>\n",
       "      <td>8.10</td>\n",
       "      <td>712</td>\n",
       "      <td>2699.958333</td>\n",
       "      <td>33667</td>\n",
       "      <td>73.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>0.1426</td>\n",
       "      <td>102.92</td>\n",
       "      <td>11.299732</td>\n",
       "      <td>14.97</td>\n",
       "      <td>667</td>\n",
       "      <td>4066.000000</td>\n",
       "      <td>4740</td>\n",
       "      <td>39.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   credit.policy             purpose  int.rate  installment  log.annual.inc  \\\n",
       "0              1  debt_consolidation    0.1189       829.10       11.350407   \n",
       "1              1         credit_card    0.1071       228.22       11.082143   \n",
       "2              1  debt_consolidation    0.1357       366.86       10.373491   \n",
       "3              1  debt_consolidation    0.1008       162.34       11.350407   \n",
       "4              1         credit_card    0.1426       102.92       11.299732   \n",
       "\n",
       "     dti  fico  days.with.cr.line  revol.bal  revol.util  inq.last.6mths  \\\n",
       "0  19.48   737        5639.958333      28854        52.1               0   \n",
       "1  14.29   707        2760.000000      33623        76.7               0   \n",
       "2  11.63   682        4710.000000       3511        25.6               1   \n",
       "3   8.10   712        2699.958333      33667        73.2               1   \n",
       "4  14.97   667        4066.000000       4740        39.5               0   \n",
       "\n",
       "   delinq.2yrs  pub.rec  not.fully.paid  \n",
       "0            0        0               0  \n",
       "1            0        0               0  \n",
       "2            0        0               0  \n",
       "3            0        0               0  \n",
       "4            1        0               0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ff41b3-cb82-4ffd-9005-8e0e9540081f",
   "metadata": {},
   "source": [
    "## Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b195948f-ea59-47b3-a640-4d9ec31f149b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting - Train MSE: 0.18\n",
      "Gradient Boosting - Test MSE: 0.2\n",
      "\n",
      "XGBoost - Train MSE: 0.06\n",
      "XGBoost - Test MSE: 0.23\n",
      "\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002362 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1595\n",
      "[LightGBM] [Info] Number of data points in the train set: 7662, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 10.934129\n",
      "LightGBM - Train MSE: 0.13\n",
      "LightGBM - Test MSE: 0.2\n",
      "\n",
      "CatBoost - Train MSE: 0.13\n",
      "CatBoost - Test MSE: 0.21\n",
      "\n",
      "AdaBoost - Train MSE: 0.25\n",
      "AdaBoost - Test MSE: 0.27\n",
      "\n",
      "Histogram-based Gradient Boosting - Train MSE: 0.12\n",
      "Histogram-based Gradient Boosting - Test MSE: 0.2\n",
      "\n",
      "Sample actual income predictions (Train): [ 46418.42390139  49340.22199464  53346.07728535 104318.40494611\n",
      "  48289.29526851]\n",
      "Sample actual income predictions (Test): [50801.79070877 61794.28943989 40188.18258602 41352.70111008\n",
      " 47600.41188874]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Load the dataset\n",
    "dataframe = pd.read_csv('loan_data.csv')  # Update with your actual path\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_columns = dataframe.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Create a LabelEncoder object\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode categorical features\n",
    "for column in categorical_columns:\n",
    "    dataframe[column] = label_encoder.fit_transform(dataframe[column])\n",
    "\n",
    "# Separate features and target variable\n",
    "features = dataframe.drop(['log.annual.inc'], axis=1)  # Features\n",
    "target = dataframe['log.annual.inc']  # Target variable (log-transformed annual income)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "features_train, features_test, target_train, target_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the models\n",
    "models = {\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100, random_state=1),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=100, random_state=1),\n",
    "    \"LightGBM\": lgb.LGBMRegressor(n_estimators=100, random_state=1),\n",
    "    \"CatBoost\": CatBoostRegressor(n_estimators=100, random_state=1, verbose=0),\n",
    "    \"AdaBoost\": AdaBoostRegressor(n_estimators=100, random_state=1),\n",
    "    \"Histogram-based Gradient Boosting\": HistGradientBoostingRegressor(max_iter=100, random_state=1)\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    model.fit(features_train, target_train)\n",
    "    y_train_pred = model.predict(features_train)\n",
    "    y_test_pred = model.predict(features_test)\n",
    "\n",
    "    # Calculate Mean Squared Error\n",
    "    train_mse = mean_squared_error(target_train, y_train_pred)\n",
    "    test_mse = mean_squared_error(target_test, y_test_pred)\n",
    "\n",
    "    print(f\"{model_name} - Train MSE: {np.round(train_mse, 2)}\")\n",
    "    print(f\"{model_name} - Test MSE: {np.round(test_mse, 2)}\\n\")\n",
    "\n",
    "# Optional: Reverse the log transformation to get actual income predictions\n",
    "actual_income_train = np.exp(y_train_pred)\n",
    "actual_income_test = np.exp(y_test_pred)\n",
    "\n",
    "print(\"Sample actual income predictions (Train):\", actual_income_train[:5])\n",
    "print(\"Sample actual income predictions (Test):\", actual_income_test[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360060a6-ccca-4b1c-bf0c-8ab25da41cdf",
   "metadata": {},
   "source": [
    "## To Apply the OneHot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e977de52-cdc5-4b53-a340-466d2e178573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9578 entries, 0 to 9577\n",
      "Data columns (total 14 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   credit.policy      9578 non-null   int64  \n",
      " 1   purpose            9578 non-null   object \n",
      " 2   int.rate           9578 non-null   float64\n",
      " 3   installment        9578 non-null   float64\n",
      " 4   log.annual.inc     9578 non-null   float64\n",
      " 5   dti                9578 non-null   float64\n",
      " 6   fico               9578 non-null   int64  \n",
      " 7   days.with.cr.line  9578 non-null   float64\n",
      " 8   revol.bal          9578 non-null   int64  \n",
      " 9   revol.util         9578 non-null   float64\n",
      " 10  inq.last.6mths     9578 non-null   int64  \n",
      " 11  delinq.2yrs        9578 non-null   int64  \n",
      " 12  pub.rec            9578 non-null   int64  \n",
      " 13  not.fully.paid     9578 non-null   int64  \n",
      "dtypes: float64(6), int64(7), object(1)\n",
      "memory usage: 1.0+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9578 entries, 0 to 9577\n",
      "Data columns (total 19 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   credit.policy               9578 non-null   int64  \n",
      " 1   int.rate                    9578 non-null   float64\n",
      " 2   installment                 9578 non-null   float64\n",
      " 3   log.annual.inc              9578 non-null   float64\n",
      " 4   dti                         9578 non-null   float64\n",
      " 5   fico                        9578 non-null   int64  \n",
      " 6   days.with.cr.line           9578 non-null   float64\n",
      " 7   revol.bal                   9578 non-null   int64  \n",
      " 8   revol.util                  9578 non-null   float64\n",
      " 9   inq.last.6mths              9578 non-null   int64  \n",
      " 10  delinq.2yrs                 9578 non-null   int64  \n",
      " 11  pub.rec                     9578 non-null   int64  \n",
      " 12  not.fully.paid              9578 non-null   int64  \n",
      " 13  purpose_credit_card         9578 non-null   bool   \n",
      " 14  purpose_debt_consolidation  9578 non-null   bool   \n",
      " 15  purpose_educational         9578 non-null   bool   \n",
      " 16  purpose_home_improvement    9578 non-null   bool   \n",
      " 17  purpose_major_purchase      9578 non-null   bool   \n",
      " 18  purpose_small_business      9578 non-null   bool   \n",
      "dtypes: bool(6), float64(6), int64(7)\n",
      "memory usage: 1.0 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "dataframe = pd.read_csv('loan_data.csv')  # Update with your actual path\n",
    "\n",
    "# Display the initial summary\n",
    "print(dataframe.info())\n",
    "\n",
    "# One-hot encoding for the 'purpose' column\n",
    "dataframe = pd.get_dummies(dataframe, columns=['purpose'], drop_first=True)\n",
    "\n",
    "# Display the updated dataframe summary\n",
    "print(dataframe.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16ba269b-5c80-4cec-919f-aca1e0276b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>credit.policy</th>\n",
       "      <th>int.rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>log.annual.inc</th>\n",
       "      <th>dti</th>\n",
       "      <th>fico</th>\n",
       "      <th>days.with.cr.line</th>\n",
       "      <th>revol.bal</th>\n",
       "      <th>revol.util</th>\n",
       "      <th>inq.last.6mths</th>\n",
       "      <th>delinq.2yrs</th>\n",
       "      <th>pub.rec</th>\n",
       "      <th>not.fully.paid</th>\n",
       "      <th>purpose_credit_card</th>\n",
       "      <th>purpose_debt_consolidation</th>\n",
       "      <th>purpose_educational</th>\n",
       "      <th>purpose_home_improvement</th>\n",
       "      <th>purpose_major_purchase</th>\n",
       "      <th>purpose_small_business</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1189</td>\n",
       "      <td>829.10</td>\n",
       "      <td>11.350407</td>\n",
       "      <td>19.48</td>\n",
       "      <td>737</td>\n",
       "      <td>5639.958333</td>\n",
       "      <td>28854</td>\n",
       "      <td>52.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1071</td>\n",
       "      <td>228.22</td>\n",
       "      <td>11.082143</td>\n",
       "      <td>14.29</td>\n",
       "      <td>707</td>\n",
       "      <td>2760.000000</td>\n",
       "      <td>33623</td>\n",
       "      <td>76.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1357</td>\n",
       "      <td>366.86</td>\n",
       "      <td>10.373491</td>\n",
       "      <td>11.63</td>\n",
       "      <td>682</td>\n",
       "      <td>4710.000000</td>\n",
       "      <td>3511</td>\n",
       "      <td>25.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1008</td>\n",
       "      <td>162.34</td>\n",
       "      <td>11.350407</td>\n",
       "      <td>8.10</td>\n",
       "      <td>712</td>\n",
       "      <td>2699.958333</td>\n",
       "      <td>33667</td>\n",
       "      <td>73.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1426</td>\n",
       "      <td>102.92</td>\n",
       "      <td>11.299732</td>\n",
       "      <td>14.97</td>\n",
       "      <td>667</td>\n",
       "      <td>4066.000000</td>\n",
       "      <td>4740</td>\n",
       "      <td>39.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   credit.policy  int.rate  installment  log.annual.inc    dti  fico  \\\n",
       "0              1    0.1189       829.10       11.350407  19.48   737   \n",
       "1              1    0.1071       228.22       11.082143  14.29   707   \n",
       "2              1    0.1357       366.86       10.373491  11.63   682   \n",
       "3              1    0.1008       162.34       11.350407   8.10   712   \n",
       "4              1    0.1426       102.92       11.299732  14.97   667   \n",
       "\n",
       "   days.with.cr.line  revol.bal  revol.util  inq.last.6mths  delinq.2yrs  \\\n",
       "0        5639.958333      28854        52.1               0            0   \n",
       "1        2760.000000      33623        76.7               0            0   \n",
       "2        4710.000000       3511        25.6               1            0   \n",
       "3        2699.958333      33667        73.2               1            0   \n",
       "4        4066.000000       4740        39.5               0            1   \n",
       "\n",
       "   pub.rec  not.fully.paid  purpose_credit_card  purpose_debt_consolidation  \\\n",
       "0        0               0                False                        True   \n",
       "1        0               0                 True                       False   \n",
       "2        0               0                False                        True   \n",
       "3        0               0                False                        True   \n",
       "4        0               0                 True                       False   \n",
       "\n",
       "   purpose_educational  purpose_home_improvement  purpose_major_purchase  \\\n",
       "0                False                     False                   False   \n",
       "1                False                     False                   False   \n",
       "2                False                     False                   False   \n",
       "3                False                     False                   False   \n",
       "4                False                     False                   False   \n",
       "\n",
       "   purpose_small_business  \n",
       "0                   False  \n",
       "1                   False  \n",
       "2                   False  \n",
       "3                   False  \n",
       "4                   False  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c381bae-1f22-445c-9849-e5b4424fb2f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
